{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to select the most appropriate algorithm for the problem. A good principle is to start with the simpler one and work your way up to more complex ones if the results are not satisfying. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from pprint import pprint\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6362, 18861)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.sparse import load_npz, hstack\n",
    "\n",
    "min_count = 5\n",
    "\n",
    "emails = pd.read_pickle('./data/emails.pkl')\n",
    "subjects_BoW = load_npz('./data/subjects_BoW.npz')\n",
    "contents_BoW = load_npz('./data/contents_BoW.npz')\n",
    "FromUsers = load_npz('./data/FromUsers.npz')\n",
    "ToUsers = load_npz('./data/ToUsers.npz')\n",
    "FromDomains = load_npz('./data/FromDomains.npz')\n",
    "ToDomains = load_npz('./data/ToDomains.npz')\n",
    "years = load_npz('./data/years.npz')\n",
    "days = load_npz('./data/days.npz')\n",
    "hours = load_npz('./data/hours.npz')\n",
    "\n",
    "# Drop columns that have less than the min count\n",
    "subjects_BoW = subjects_BoW[:,subjects_BoW.sum(0).A[0] > min_count]\n",
    "contents_BoW = contents_BoW[:,contents_BoW.sum(0).A[0] > min_count]\n",
    "FromUsers = FromUsers[:,FromUsers.sum(0).A[0] > min_count]\n",
    "ToUsers = ToUsers[:,ToUsers.sum(0).A[0] > min_count]\n",
    "FromDomains = FromDomains[:,FromDomains.sum(0).A[0] > min_count]\n",
    "ToDomains = ToDomains[:,ToDomains.sum(0).A[0] > min_count]\n",
    "\n",
    "# Stack the data altogether\n",
    "processed_data = hstack([subjects_BoW, contents_BoW, FromUsers, ToUsers, FromDomains, ToDomains, years, days, hours], format='csr', dtype=float)\n",
    "del subjects_BoW; del contents_BoW; del FromUsers; del ToUsers; del FromDomains; del ToDomains\n",
    "\n",
    "processed_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data shuffling and creation of test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = processed_data, emails['label'][:6362].values\n",
    "del processed_data\n",
    "del emails\n",
    "\n",
    "indexes = list(range(X.shape[0]))\n",
    "random.seed(1)\n",
    "random.shuffle(indexes)\n",
    "\n",
    "X, y = X[indexes], y[indexes]\n",
    "cutoff = int(X.shape[0]*0.7)\n",
    "\n",
    "X_train_valid, y_train_valid = X[:cutoff], y[:cutoff]\n",
    "X_test, y_test = X[cutoff:], y[cutoff:]\n",
    "\n",
    "del X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning algorithm, here, the logistic regression has been selected for its low complexity and more than interesting score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "parameters:\n",
      "{'C': (0.1, 0.5, 1.0),\n",
      " 'class_weight': ('balanced', None),\n",
      " 'fit_intercept': (True, False),\n",
      " 'max_iter': (300,),\n",
      " 'penalty': ('l2', 'l1'),\n",
      " 'tol': (0.01,)}\n",
      "done in 72.835s\n",
      "\n",
      "Best score for validation set : 0.897\n"
     ]
    }
   ],
   "source": [
    "# Initialisation\n",
    "LR = LogisticRegression()\n",
    "\n",
    "parameters = {\n",
    "        'penalty': ('l2', 'l1'),\n",
    "        'class_weight':('balanced', None),\n",
    "        'tol':(1e-2, ),\n",
    "        'C': (0.1, 0.5, 1.0),\n",
    "        'fit_intercept': (True, False),\n",
    "        'max_iter':(300,)\n",
    "    }\n",
    "\n",
    "grid_search = GridSearchCV(LR, parameters,  verbose=0, iid=True, cv=4, n_jobs=-1, return_train_score=False, scoring='f1')\n",
    "\n",
    "print(\"Performing grid search...\")\n",
    "print(\"parameters:\")\n",
    "pprint(parameters)\n",
    "t0 = time()\n",
    "grid_search.fit(X_train_valid, y_train_valid)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()\n",
    "best_LR = grid_search.best_estimator_\n",
    "score_train = grid_search.best_score_\n",
    "print(\"Best score for validation set : %0.3f\" % score_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "parameters:\n",
      "{'C': (0.1, 0.5, 1.0),\n",
      " 'class_weight': ('balanced', None),\n",
      " 'fit_intercept': (True, False),\n",
      " 'max_iter': (200, 100),\n",
      " 'tol': (0.01,)}\n",
      "done in 16.219s\n",
      "\n",
      "Best score for validation set : 0.892\n"
     ]
    }
   ],
   "source": [
    "# Initialisation\n",
    "SVM = LinearSVC()\n",
    "\n",
    "parameters = {\n",
    "        'class_weight':('balanced', None),\n",
    "        'tol':(1e-2, ),\n",
    "        'C': (0.1, 0.5, 1.0),\n",
    "        'fit_intercept': (True, False),\n",
    "        'max_iter':(200,100)\n",
    "    }\n",
    "\n",
    "grid_search = GridSearchCV(SVM, parameters,  verbose=0, iid=True, cv=4, n_jobs=-1, return_train_score=False, scoring='f1')\n",
    "\n",
    "print(\"Performing grid search...\")\n",
    "print(\"parameters:\")\n",
    "pprint(parameters)\n",
    "t0 = time()\n",
    "grid_search.fit(X_train_valid, y_train_valid)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()\n",
    "best_SVM = grid_search.best_estimator_\n",
    "score_train = grid_search.best_score_\n",
    "print(\"Best score for validation set : %0.3f\" % score_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good start for scoring a classifier is to analyse their precision, recall and f1-score which is the combination of the two. Indeed, here the classes are a bid imbalanced, so this could have a good precision by just predicting the most represented class.\n",
    "\n",
    "The f1-score usually gives a balanced scoring by giving as much importance to the minority class than the majority one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------Best Logistic Regression--------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.96      0.97      1476\n",
      "          1       0.88      0.92      0.90       433\n",
      "\n",
      "avg / total       0.95      0.95      0.95      1909\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----------------------Best SVM----------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.96      0.97      1476\n",
      "          1       0.88      0.90      0.89       433\n",
      "\n",
      "avg / total       0.95      0.95      0.95      1909\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = best_LR.predict(X_test)\n",
    "print('--------------Best Logistic Regression--------------')\n",
    "print(classification_report(y_test, predictions))\n",
    "print('\\n\\n')\n",
    "\n",
    "print('----------------------Best SVM----------------------')\n",
    "predictions = best_SVM.predict(X_test)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the algorithm seems to be perfectly fine for the problem. We could add some boosting to grab the last bit of precision, but it would cost more calculation at prediction time, so more money overall. Plus, the logistic regression is easy to parallelize if needs be."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
